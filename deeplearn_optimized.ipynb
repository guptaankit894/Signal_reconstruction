{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ca08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20765a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899feaf7-71a5-4020-8f4f-e184324bb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d439ee2-50e5-4010-be63-03ff6adc9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6278aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f613e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984a1beb-069c-4d55-b30c-874599dc078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ad7485-9fdd-41fd-8d8b-144bb23fbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06c4520-d8bc-4e88-81b6-2d046f40362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8b4ed9-a06d-4b21-bda5-82e1f10946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3ae97d-cbcf-4026-9a7f-30166be300f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e92fb0-e4e9-40c7-8dfd-94b1f9233e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9603fda-df4d-45ed-96da-6093a3a77527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39e93e6-0312-4f1d-9b6c-70ee001158c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8c9d9a-5eca-41a6-80da-e9bb16d59143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb88e61-4f9e-4e9c-a138-1caaaf590a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\AppData\\Local\\Temp\\ipykernel_24952\\3292659327.py:6: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  ecg_2=ecg_model.state_dict(checkpoint[\"model_state_dict\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder1D(\n",
       "  (encoder): Encoder(\n",
       "    (enc1): DeformableConv1D(\n",
       "      (offsets): Conv1d(1, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(1, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc2): DeformableConv1D(\n",
       "      (offsets): Conv1d(8, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(8, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (conv2): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc3): DeformableConv1D(\n",
       "      (offsets): Conv1d(16, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(16, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (conv3): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc4): DeformableConv1D(\n",
       "      (offsets): Conv1d(32, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(32, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (conv4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc5): DeformableConv1D(\n",
       "      (offsets): Conv1d(64, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(64, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (conv5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): Swish()\n",
       "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (global_avg): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dec1): DeformableConv1D(\n",
       "      (offsets): Conv1d(16, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(16, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (dec2): DeformableConv1D(\n",
       "      (offsets): Conv1d(8, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (deform_conv): DeformConv2d(8, 1, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode='linear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b763b755-7af2-4e17-9e78-91fd4c26d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7d726e-326d-4496-91c4-1e2178c6f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e764ea9d-f028-48f7-bd12-0e6113f8bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ca6e63-fadd-4066-802f-1baae478cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d738625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dcfd0-a045-4408-92a2-f992e13c99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0405b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9de9963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b996ff-7641-4f20-bbbf-8423b02661a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\AppData\\Local\\Programs\\Python\\Python313\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28501874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc377bc7-e6cf-4524-a556-0a5c31fc8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a030df-f882-42e0-8f3f-637d82a15c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c89d20-51c2-41b4-b38d-50f77a528a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b9063-370f-4857-959a-c58f6a982ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bcff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Enable cuDNN Benchmarking for faster training on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set device for computation (Ensure proper GPU utilization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fs = 100\n",
    "\n",
    "# Load dataset\n",
    "mat_Res = si.loadmat('u_lma.mat')\n",
    "mat_results = mat_Res['a']\n",
    "\n",
    "# Custom Dataset (if needed)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = CustomDataset(mat_results)\n",
    "\n",
    "# Optimized DataLoader with num_workers and pin_memory for better performance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sample Model (Ensure it runs on multiple GPUs if available)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop (with memory management)\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(output, batch[:, 0])  # Example loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Empty CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
